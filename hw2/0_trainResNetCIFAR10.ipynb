{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import argparse\n",
    "from matplotlib import  pyplot as plt\n",
    "from utils import *\n",
    "from resnetCIFAR10 import ResNet18, ResNet50\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "dataDir = f'../db'\n",
    "modelDir = f'./model'\n",
    "logDir = f'./log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train samples: 50000 | # test samples:10000\n",
      "per image size: 32*32 | per image channel:3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1) \n",
    "random.seed(1) \n",
    "# reference for RandomCrop and RandomHorizontalFlip\n",
    "# https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(dataDir, train=True,  download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(dataDir, train=False,  download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "nTrainSamples, width, height, channel = trainset.data.shape\n",
    "nTestSamples, width, height, channel = testset.data.shape\n",
    "print(f'# train samples: {nTrainSamples} | # test samples:{nTestSamples}')\n",
    "print(f'per image size: {width}*{height} | per image channel:{channel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint from './model/cifar10-resnet-checkpoint.pth.tar'\n",
      "For the loaded net: testing loss: 0.6053 | testing accuracy:[0.9465]\n",
      "Recorded          : testing loss: 0.6053 | testing accuracy:[0.9465]\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "netname=f'cifar10-resnet'\n",
    "# choose optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "logFilePath= f'{logDir}/{netname}'\n",
    "logger = Logger(logFilePath)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "checkpointPath = f'{modelDir}/{netname}-checkpoint.pth.tar'\n",
    "netclf = TrainAndTest(net, trainloader, testloader, \n",
    "                           criterion, optimizer, netname=netname)\n",
    "netclf.build(start_epoch=0, total_epochs=200, checkpointPath=checkpointPath, \n",
    "                           logger=logger, modelDir=modelDir, vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, accuracy_score\n",
    "# def _test(net, testloader, criterion, vectorize):\n",
    "#     net.eval()\n",
    "#     test_accuracy = 0.0\n",
    "#     total = 0\n",
    "#     batch_size = testloader.batch_size\n",
    "#     Ypred = []\n",
    "#     for i, (images, labels) in enumerate(testloader):\n",
    "#         if vectorize:\n",
    "#             images = images.view([images.shape[0], -1])\n",
    "#         images, labels = images.cuda(), labels.cuda()\n",
    "#         output = net(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         predicted = output.data.max(1)[1]\n",
    "#         Ypred += predicted\n",
    "#         accuracy = float(predicted.eq(labels.data).sum())\n",
    "#         test_accuracy += accuracy\n",
    "#         total += labels.size(0)\n",
    "#     test_accuracy_epoch = test_accuracy / total\n",
    "#     test_loss_epoch = loss.item()\n",
    "#     return test_loss_epoch, test_accuracy_epoch, Ypred, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ytest = torch.tensor(testset.targets)\n",
    "# test_loss, test_accuracy, Ypred, total = _test(net, testloader, criterion, vectorize=False)\n",
    "# accuracy_score([i.cpu() for i in Ypred], Ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
